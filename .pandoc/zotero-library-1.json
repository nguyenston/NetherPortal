[
  {"id":"buiNonnegativeMatrixFactorization","author":[{"family":"Bui","given":"Minh Thu"}],"citation-key":"buiNonnegativeMatrixFactorization","language":"en","source":"Zotero","title":"Non-negative Matrix Factorization in STARE method","type":"manuscript"},
  {"id":"chenSelectingOptimalDecisions","author":[{"family":"Chen","given":"Ruidi"},{"family":"Paschalidis","given":"Ioannis Ch"}],"citation-key":"chenSelectingOptimalDecisions","language":"en","source":"Zotero","title":"Selecting Optimal Decisions via Distributionally Robust Nearest-Neighbor Regression","type":"article-journal"},
  {"id":"chenSelectingOptimalDecisions2019","abstract":"This paper develops a prediction-based prescriptive model for optimal decision\nmaking that (i) predicts the outcome under each action using a robust\nnonlinear model, and (ii) adopts a randomized prescriptive policy determined\nby the predicted outcomes. The predictive model combines a new regularized\nregression technique, which was developed using Distributionally Robust\nOptimization (DRO) with an ambiguity set constructed from the Wasserstein\nmetric, with the K-Nearest Neighbors (K-NN) regression, which helps to\ncapture the nonlinearity embedded in the data. We show theoretical results\nthat guarantee the out-of-sample performance of the predictive model, and\nprove the optimality of the randomized policy in terms of the expected true\nfuture outcome. We demonstrate the proposed methodology on a hypertension\ndataset, showing that our prescribed treatment leads to a larger reduction in\nthe systolic blood pressure compared to a series of alternatives. A clinically\nmeaningful threshold level used to activate the randomized policy is also\nderived under a sub-Gaussian assumption on the predicted outcome.","accessed":{"date-parts":[["2024",1,14]]},"author":[{"family":"Chen","given":"Ruidi"},{"family":"Paschalidis","given":"Ioannis"}],"citation-key":"chenSelectingOptimalDecisions2019","container-title":"Advances in Neural Information Processing Systems","issued":{"date-parts":[["2019"]]},"publisher":"Curran Associates, Inc.","source":"Neural Information Processing Systems","title":"Selecting Optimal Decisions via Distributionally Robust Nearest-Neighbor Regression","type":"paper-conference","URL":"https://proceedings.neurips.cc/paper/2019/hash/8efb100a295c0c690931222ff4467bb8-Abstract.html","volume":"32"},
  {"id":"giladAutomatedApproachDetermining2020","abstract":"Non-negative matrix factorization (NMF) is a popular method for finding a low rank approximation of a matrix, thereby revealing the latent components behind it. In genomics, NMF is widely used to interpret mutation data and derive the underlying mutational processes and their activities. A key challenge in the use of NMF is determining the number of components, or rank of the factorization. Here we propose a novel method, CV2K, to choose this number automatically from data that is based on a detailed cross validation procedure combined with a parsimony consideration. We apply our method for mutational signature analysis and demonstrate its utility on both simulated and real data sets. In comparison to previous approaches, some of which involve human assessment, CV2K leads to improved predictions across a wide range of data sets.","accessed":{"date-parts":[["2024",1,28]]},"author":[{"family":"Gilad","given":"Gal"},{"family":"Sason","given":"Itay"},{"family":"Sharan","given":"Roded"}],"citation-key":"giladAutomatedApproachDetermining2020","container-title":"Machine Learning: Science and Technology","container-title-short":"Mach. Learn.: Sci. Technol.","DOI":"10.1088/2632-2153/abc60a","ISSN":"2632-2153","issue":"1","issued":{"date-parts":[["2020",12]]},"language":"en","page":"015013","publisher":"IOP Publishing","source":"Institute of Physics","title":"An automated approach for determining the number of components in non-negative matrix factorization with application to mutational signature learning","type":"article-journal","URL":"https://dx.doi.org/10.1088/2632-2153/abc60a","volume":"2"},
  {"id":"huPersonalizedHypertensionTreatment2023","abstract":"Hypertension is a prevalent cardiovascular disease with severe longer-term implications. Conventional management based on clinical guidelines does not facilitate personalized treatment that accounts for a richer set of patient characteristics.","accessed":{"date-parts":[["2024",1,14]]},"author":[{"family":"Hu","given":"Yang"},{"family":"Huerta","given":"Jasmine"},{"family":"Cordella","given":"Nicholas"},{"family":"Mishuris","given":"Rebecca G."},{"family":"Paschalidis","given":"Ioannis Ch."}],"citation-key":"huPersonalizedHypertensionTreatment2023","container-title":"BMC Medical Informatics and Decision Making","container-title-short":"BMC Medical Informatics and Decision Making","DOI":"10.1186/s12911-023-02137-z","ISSN":"1472-6947","issue":"1","issued":{"date-parts":[["2023",3,1]]},"page":"44","source":"BioMed Central","title":"Personalized hypertension treatment recommendations by a data-driven model","type":"article-journal","URL":"https://doi.org/10.1186/s12911-023-02137-z","volume":"23"},
  {"id":"liRobustStructurallyAware","abstract":"Mixture models are often used to discover unobserved groups with real-world interpretations that generate observed data. However, mixture model inference is usually ill-defined a priori because the assumed observation model is only an approximation to the true data-generating process and the number of groups is unknown. Thus, as the number of observations increases, rather than obtaining better inferences, the opposite occurs: the data is explained by adding spurious groups that compensate for the shortcomings of the observation model. However, there are two important sources of prior knowledge that we can exploit to obtain well-defined results no matter the dataset size: causal information (e.g., knowing that the latent groups cause the observed signal but not vice-versa) and a rough sense of how wrong the observation model is (e.g., based on small amounts of expert-labeled data or some understanding of the data-generating process). We propose a new model selection criteria that, while model-based, uses this available knowledge to obtain mixture model inferences that are robust to misspecification of the observation model and account for known causal structure. We provide theoretical support for our approach by proving a first-of-its-kind consistency result under intuitive assumptions. Simulation studies and an application to flow cytometry data demonstrate our model selection criteria is consistently finds the correct number of components.","author":[{"family":"Li","given":"J"}],"citation-key":"liRobustStructurallyAware","language":"en","source":"Zotero","title":"Robust, Structurally Aware Model Selection for Mixtures","type":"manuscript"},
  {"id":"millerMixtureModelsPrior2018","abstract":"A natural Bayesian approach for mixture models with an unknown number of components is to take the usual finite mixture model with symmetric Dirichlet weights, and put a prior on the number of components—that is, to use a mixture of finite mixtures (MFM). The most commonly used method of inference for MFMs is reversible jump Markov chain Monte Carlo, but it can be nontrivial to design good reversible jump moves, especially in high-dimensional spaces. Meanwhile, there are samplers for Dirichlet process mixture (DPM) models that are relatively simple and are easily adapted to new applications. It turns out that, in fact, many of the essential properties of DPMs are also exhibited by MFMs—an exchangeable partition distribution, restaurant process, random measure representation, and stick-breaking representation—and crucially, the MFM analogues are simple enough that they can be used much like the corresponding DPM properties. Consequently, many of the powerful methods developed for inference in DPMs can be directly applied to MFMs as well; this simplifies the implementation of MFMs and can substantially improve mixing. We illustrate with real and simulated data, including high-dimensional gene expression data used to discriminate cancer subtypes. Supplementary materials for this article are available online.","accessed":{"date-parts":[["2024",1,12]]},"author":[{"family":"Miller","given":"Jeffrey W."},{"family":"Harrison","given":"Matthew T."}],"citation-key":"millerMixtureModelsPrior2018","container-title":"Journal of the American Statistical Association","container-title-short":"Journal of the American Statistical Association","DOI":"10.1080/01621459.2016.1255636","ISSN":"0162-1459, 1537-274X","issue":"521","issued":{"date-parts":[["2018",1,2]]},"language":"en","page":"340-356","source":"DOI.org (Crossref)","title":"Mixture Models With a Prior on the Number of Components","type":"article-journal","URL":"https://www.tandfonline.com/doi/full/10.1080/01621459.2016.1255636","volume":"113"},
  {"id":"negreaTuningStochasticGradient2023","abstract":"The tuning of stochastic gradient algorithms (SGAs) for optimization and sampling is often based on heuristics and trial-and-error rather than generalizable theory. We address this theory--practice gap by characterizing the large-sample statistical asymptotics of SGAs via a joint step-size--sample-size scaling limit. We show that iterate averaging with a large fixed step size is robust to the choice of tuning parameters and asymptotically has covariance proportional to that of the MLE sampling distribution. We also prove a Bernstein--von Mises-like theorem to guide tuning, including for generalized posteriors that are robust to model misspecification. Numerical experiments validate our results and recommendations in realistic finite-sample regimes. Our work lays the foundation for a systematic analysis of other stochastic gradient Markov chain Monte Carlo algorithms for a wide range of models.","accessed":{"date-parts":[["2024",1,12]]},"author":[{"family":"Negrea","given":"Jeffrey"},{"family":"Yang","given":"Jun"},{"family":"Feng","given":"Haoyue"},{"family":"Roy","given":"Daniel M."},{"family":"Huggins","given":"Jonathan H."}],"citation-key":"negreaTuningStochasticGradient2023","DOI":"10.48550/arXiv.2207.12395","issued":{"date-parts":[["2023",7,20]]},"number":"arXiv:2207.12395","publisher":"arXiv","source":"arXiv.org","title":"Tuning Stochastic Gradient Algorithms for Statistical Inference via Large-Sample Asymptotics","type":"article","URL":"http://arxiv.org/abs/2207.12395"},
  {"id":"pelizzolaModelSelectionRobust2023","abstract":"The spectrum of mutations in a collection of cancer genomes can be described by a mixture of a few mutational signatures. The mutational signatures can be found using non-negative matrix factorization (NMF). To extract the mutational signatures we have to assume a distribution for the observed mutational counts and a number of mutational signatures. In most applications, the mutational counts are assumed to be Poisson distributed, and the rank is chosen by comparing the fit of several models with the same underlying distribution and different values for the rank using classical model selection procedures. However, the counts are often overdispersed, and thus the Negative Binomial distribution is more appropriate.","accessed":{"date-parts":[["2024",1,28]]},"author":[{"family":"Pelizzola","given":"Marta"},{"family":"Laursen","given":"Ragnhild"},{"family":"Hobolth","given":"Asger"}],"citation-key":"pelizzolaModelSelectionRobust2023","container-title":"BMC Bioinformatics","container-title-short":"BMC Bioinformatics","DOI":"10.1186/s12859-023-05304-1","ISSN":"1471-2105","issue":"1","issued":{"date-parts":[["2023",5,8]]},"page":"187","source":"BioMed Central","title":"Model selection and robust inference of mutational signatures using Negative Binomial non-negative matrix factorization","type":"article-journal","URL":"https://doi.org/10.1186/s12859-023-05304-1","volume":"24"}
]
