[
  {"id":"chenSelectingOptimalDecisions2019","abstract":"This paper develops a prediction-based prescriptive model for optimal decision\nmaking that (i) predicts the outcome under each action using a robust\nnonlinear model, and (ii) adopts a randomized prescriptive policy determined\nby the predicted outcomes. The predictive model combines a new regularized\nregression technique, which was developed using Distributionally Robust\nOptimization (DRO) with an ambiguity set constructed from the Wasserstein\nmetric, with the K-Nearest Neighbors (K-NN) regression, which helps to\ncapture the nonlinearity embedded in the data. We show theoretical results\nthat guarantee the out-of-sample performance of the predictive model, and\nprove the optimality of the randomized policy in terms of the expected true\nfuture outcome. We demonstrate the proposed methodology on a hypertension\ndataset, showing that our prescribed treatment leads to a larger reduction in\nthe systolic blood pressure compared to a series of alternatives. A clinically\nmeaningful threshold level used to activate the randomized policy is also\nderived under a sub-Gaussian assumption on the predicted outcome.","accessed":{"date-parts":[["2024",1,14]]},"author":[{"family":"Chen","given":"Ruidi"},{"family":"Paschalidis","given":"Ioannis"}],"citation-key":"chenSelectingOptimalDecisions2019","container-title":"Advances in Neural Information Processing Systems","issued":{"date-parts":[["2019"]]},"publisher":"Curran Associates, Inc.","source":"Neural Information Processing Systems","title":"Selecting Optimal Decisions via Distributionally Robust Nearest-Neighbor Regression","type":"paper-conference","URL":"https://proceedings.neurips.cc/paper/2019/hash/8efb100a295c0c690931222ff4467bb8-Abstract.html","volume":"32"},
  {"id":"huPersonalizedHypertensionTreatment2023","abstract":"Hypertension is a prevalent cardiovascular disease with severe longer-term implications. Conventional management based on clinical guidelines does not facilitate personalized treatment that accounts for a richer set of patient characteristics.","accessed":{"date-parts":[["2024",1,14]]},"author":[{"family":"Hu","given":"Yang"},{"family":"Huerta","given":"Jasmine"},{"family":"Cordella","given":"Nicholas"},{"family":"Mishuris","given":"Rebecca G."},{"family":"Paschalidis","given":"Ioannis Ch."}],"citation-key":"huPersonalizedHypertensionTreatment2023","container-title":"BMC Medical Informatics and Decision Making","container-title-short":"BMC Medical Informatics and Decision Making","DOI":"10.1186/s12911-023-02137-z","ISSN":"1472-6947","issue":"1","issued":{"date-parts":[["2023",3,1]]},"page":"44","source":"BioMed Central","title":"Personalized hypertension treatment recommendations by a data-driven model","type":"article-journal","URL":"https://doi.org/10.1186/s12911-023-02137-z","volume":"23"},
  {"id":"liRobustStructurallyAware","author":[{"family":"Li","given":"J"}],"citation-key":"liRobustStructurallyAware","language":"en","source":"Zotero","title":"Robust, Structurally Aware Model Selection for Mixtures","type":"article-journal"},
  {"id":"millerMixtureModelsPrior2018","abstract":"A natural Bayesian approach for mixture models with an unknown number of components is to take the usual finite mixture model with symmetric Dirichlet weights, and put a prior on the number of components—that is, to use a mixture of finite mixtures (MFM). The most commonly used method of inference for MFMs is reversible jump Markov chain Monte Carlo, but it can be nontrivial to design good reversible jump moves, especially in high-dimensional spaces. Meanwhile, there are samplers for Dirichlet process mixture (DPM) models that are relatively simple and are easily adapted to new applications. It turns out that, in fact, many of the essential properties of DPMs are also exhibited by MFMs—an exchangeable partition distribution, restaurant process, random measure representation, and stick-breaking representation—and crucially, the MFM analogues are simple enough that they can be used much like the corresponding DPM properties. Consequently, many of the powerful methods developed for inference in DPMs can be directly applied to MFMs as well; this simplifies the implementation of MFMs and can substantially improve mixing. We illustrate with real and simulated data, including high-dimensional gene expression data used to discriminate cancer subtypes. Supplementary materials for this article are available online.","accessed":{"date-parts":[["2024",1,12]]},"author":[{"family":"Miller","given":"Jeffrey W."},{"family":"Harrison","given":"Matthew T."}],"citation-key":"millerMixtureModelsPrior2018","container-title":"Journal of the American Statistical Association","container-title-short":"Journal of the American Statistical Association","DOI":"10.1080/01621459.2016.1255636","ISSN":"0162-1459, 1537-274X","issue":"521","issued":{"date-parts":[["2018",1,2]]},"language":"en","page":"340-356","source":"DOI.org (Crossref)","title":"Mixture Models With a Prior on the Number of Components","type":"article-journal","URL":"https://www.tandfonline.com/doi/full/10.1080/01621459.2016.1255636","volume":"113"},
  {"id":"negreaTuningStochasticGradient2023","abstract":"The tuning of stochastic gradient algorithms (SGAs) for optimization and sampling is often based on heuristics and trial-and-error rather than generalizable theory. We address this theory--practice gap by characterizing the large-sample statistical asymptotics of SGAs via a joint step-size--sample-size scaling limit. We show that iterate averaging with a large fixed step size is robust to the choice of tuning parameters and asymptotically has covariance proportional to that of the MLE sampling distribution. We also prove a Bernstein--von Mises-like theorem to guide tuning, including for generalized posteriors that are robust to model misspecification. Numerical experiments validate our results and recommendations in realistic finite-sample regimes. Our work lays the foundation for a systematic analysis of other stochastic gradient Markov chain Monte Carlo algorithms for a wide range of models.","accessed":{"date-parts":[["2024",1,12]]},"author":[{"family":"Negrea","given":"Jeffrey"},{"family":"Yang","given":"Jun"},{"family":"Feng","given":"Haoyue"},{"family":"Roy","given":"Daniel M."},{"family":"Huggins","given":"Jonathan H."}],"citation-key":"negreaTuningStochasticGradient2023","DOI":"10.48550/arXiv.2207.12395","issued":{"date-parts":[["2023",7,20]]},"number":"arXiv:2207.12395","publisher":"arXiv","source":"arXiv.org","title":"Tuning Stochastic Gradient Algorithms for Statistical Inference via Large-Sample Asymptotics","type":"article","URL":"http://arxiv.org/abs/2207.12395"}
]
