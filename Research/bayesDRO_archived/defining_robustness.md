---
title: Defining robustness
---

With the definitions defined [[bayesDRO_summary#Framework description|here]], we want to solve the minimax problem:
$$
Q^\star=\mathop{\mathrm{argmin}}_{Q\in\mathcal{Q}}\ \mathbb{E}_{\theta \sim Q}\left\{\mathop{\mathrm{sup}}_{P\in\Omega(P_{X})}\ \mathbb{E}_{x\sim P}[L(\theta,x)]\right\} +\alpha D_{KL}(Q|Q_{0}).\tag{MAIN}
$$
We want to infer the posterior $Q^\star$ over parameter $\theta$ so as to be **robust** against **distributional shifts** in the the data $X$ (distributionally robust optimization - DRO). In order to solidify this notion of **"robustness"**, it is useful to specify several scenarios of distributional shifts and what it means to be robust against these adversary.

# Huber's contamination model

Consider the training data $X=\{x_{i}\}_{i=1:n}\overset{\text{i.i.d.}}{\sim}\tilde{P}=(1-\epsilon)P_{\star}+\epsilon D$, where $\tilde{P}$, the contaminated distribution, is the convex combination of $P_{\star}$, the true data generating distribution, and $D$, some contaminating distribution.

![[contaminated_density.svg|90%]]
Fig. Example of a contaminated density $0.9\mathcal{N}(3,1)+0.1\mathcal{N}(10,1)$

We want the inference of posterior $Q_{\star}$ to be insensitive to data generated by the contaminating distribution $D$. This coincides somewhat with insensitivity to tail specification.

> [!tip] Remark
> In the context of DRO, the "distributional shift" lies in the difference between the contaminated training data generating distribution $\tilde{P}$ and the true/test data generating distribution $P_{\star}$. The **hostility** of this kind of shift is relatively **mild**, though.

# Worst case sub-population

Modern datasets usually contain heterogeneous subpopulations and we want to perform consistently well across all of these. Many methods that maximize average performance might perform very poorly on underrepresented data. For example, speech or facial recognition might not perform as well on minority groups.

Interestingly, this notion of "robustness" somewhat (but not completely) goes against that of the Huber's contamination model, as you'd like to perform well on even outlying "hard" instances, possibly at the cost of performance on the majority rest of the data.

> [!tip] Remark
> In the context of DRO, the "distributional shift" lies in the fact that in reality/testing, these "rare" instances might be more common than the training data might suggests.
